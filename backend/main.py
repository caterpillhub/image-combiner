import os
import uuid
import json
import shutil
import tempfile
import time
from typing import Dict, List, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(title="ImageCombiner API", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# In-memory storage (replacing MongoDB for simplicity)
downloads_db = {}

# Models
class DownloadRequest(BaseModel):
    name: Optional[str] = None
    project_name: Optional[str] = "ImageCombiner"

class DownloadResponse(BaseModel):
    download_id: str
    message: str

# Python code template for image processing
PYTHON_CODE_TEMPLATE = '''#!/usr/bin/env python3
"""
ImageCombiner - Advanced Image Compression Tool
Processes multiple images in a folder and compresses them into a single frame.
Perfect for creative professionals who need efficient image management.

Author: ImageCombiner Team
License: MIT
"""
from PIL import Image
import os
from pathlib import Path

def combine_images_16_9(input_folder=r"images/", output_image="combined_16_9.jpg", 
                       final_width=1920, final_height=1080, quality=85):
    """
    Combine multiple images into a single 16:9 aspect ratio image.
    
    Args:
        input_folder: Path to folder containing images
        output_image: Output filename
        final_width: Final image width (default 1920)
        final_height: Final image height (default 1080)
        quality: JPEG quality (1-100, default 85)
    """
    
    # Create input folder if it doesn't exist
    Path(input_folder).mkdir(exist_ok=True)
    
    # Get list of supported image formats
    supported_formats = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')
    
    # Get sorted list of images
    try:
        all_files = os.listdir(input_folder)
        images = sorted([img for img in all_files 
                        if img.lower().endswith(supported_formats)])
    except FileNotFoundError:
        print(f"‚ùå Error: Folder '{input_folder}' not found!")
        return False
    
    if not images:
        print(f"‚ùå No supported images found in '{input_folder}'")
        print(f"Supported formats: {', '.join(supported_formats)}")
        return False
    
    print(f"üìÅ Found {len(images)} images to combine")
    
    # Calculate strip width dynamically based on number of images
    strip_width = final_width // len(images)
    
    # Ensure minimum width
    if strip_width < 10:
        print(f"‚ö†Ô∏è  Warning: Too many images ({len(images)}). Each strip will be very narrow.")
        strip_width = 10
    
    print(f"üîß Each image will be resized to {strip_width}x{final_height}")
    
    # Process images
    processed_images = []
    failed_images = []
    
    for i, img_name in enumerate(images):
        try:
            img_path = os.path.join(input_folder, img_name)
            print(f"üì∑ Processing {i+1}/{len(images)}: {img_name}")
            
            # Open and process image
            with Image.open(img_path) as img:
                # Convert to RGB if needed (handles PNG with transparency, etc.)
                if img.mode != 'RGB':
                    if img.mode == 'RGBA':
                        # Create white background for transparent images
                        background = Image.new('RGB', img.size, (255, 255, 255))
                        background.paste(img, mask=img.split()[-1])
                        img = background
                    else:
                        img = img.convert('RGB')
                
                # Resize image to strip dimensions
                # Use LANCZOS for high quality resizing
                img_resized = img.resize((strip_width, final_height), Image.Resampling.LANCZOS)
                processed_images.append(img_resized)
                
        except Exception as e:
            print(f"‚ùå Failed to process {img_name}: {str(e)}")
            failed_images.append(img_name)
            continue
    
    if not processed_images:
        print("‚ùå No images were successfully processed!")
        return False
    
    if failed_images:
        print(f"‚ö†Ô∏è  {len(failed_images)} images failed to process: {', '.join(failed_images)}")
    
    # Create combined image
    print(f"üé® Creating combined image ({final_width}x{final_height})")
    combined_img = Image.new("RGB", (final_width, final_height), color=(255, 255, 255))
    
    # Paste images side by side
    x_offset = 0
    for img in processed_images:
        combined_img.paste(img, (x_offset, 0))
        x_offset += img.width
    
    # Fill remaining space with the last image if needed
    if x_offset < final_width and processed_images:
        remaining_width = final_width - x_offset
        if remaining_width > 0:
            # Stretch the last image to fill remaining space
            last_img = processed_images[-1]
            stretched_img = last_img.resize((remaining_width, final_height), Image.Resampling.LANCZOS)
            combined_img.paste(stretched_img, (x_offset, 0))
    
    # Save output
    try:
        combined_img.save(output_image, "JPEG", quality=quality, optimize=True)
        print(f"‚úÖ Combined image saved successfully as '{output_image}'")
        print(f"üìä Final size: {final_width}x{final_height} pixels")
        print(f"üíæ File saved with {quality}% quality")
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to save output image: {str(e)}")
        return False

def main():
    """Main function with example usage"""
    
    # Example usage with different configurations
    print("üöÄ Starting image combination process...")
    
    # Basic usage
    success = combine_images_16_9()
    
    if success:
        print("\\nüéâ Process completed successfully!")
    else:
        print("\\nüí° Tips:")
        print("- Make sure the 'images/' folder exists")
        print("- Add some image files (.jpg, .png, .bmp, .tiff, .webp)")
        print("- Check that images are not corrupted")
        print("- Ensure you have write permissions in the current directory")

if __name__ == "__main__":
    main()
'''

REQUIREMENTS_TXT = '''Pillow>=10.0.0
numpy>=1.24.0
'''

README_MD = '''# üé® Image Combiner

Fast parallel image combiner that creates panoramic 16:9 images from folders.

[![Python 3.7+](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/downloads/)
[![Performance](https://img.shields.io/badge/Performance-4x%20Faster-brightgreen.svg)](#performance)

## ‚ú® Features

- üåà **Multiple Formats** - JPG, PNG, BMP, TIFF, WebP
- üíæ **Memory Efficient** - Handles large batches without issues

## üöÄ Quick Start

### Install
```bash
pip install Pillow
```

### Command Line
```python
python imagecombiner.py  # Processes 'images/' folder
```

## üîß Key Parameters

```python
combine_images_16_9(
    input_folder,           # Folder path
    output_image,           # Output filename  
    final_width=1920,       # Output width
    final_height=1080,      # Output height
    quality=85              # JPEG quality (1-100)
)
```

## üìÅ Input/Output

**Input**: Folder with images  
**Output**: Single JPEG panoramic image (16:9 aspect ratio)  
**Formats**: JPG, PNG, BMP, TIFF, WebP ‚Üí JPEG  

**Example**: Process movie frames into a single panoramic image
'''

@app.get("/")
async def root():
    return {"message": "ImageCombiner API - Ready to serve creative professionals!", "status": "running"}

@app.post("/api/download", response_model=DownloadResponse)
async def create_download_package(request: DownloadRequest):
    """Create a download package with the ImageCombiner code."""
    try:
        # Generate unique download ID
        download_id = str(uuid.uuid4())
        
        # Create temporary directory for the package
        temp_dir = tempfile.mkdtemp(prefix="imagepack_")
        package_dir = os.path.join(temp_dir, "ImagePack")
        os.makedirs(package_dir, exist_ok=True)
        
        logger.info(f"Created temp directory: {temp_dir}")
        
        # Write files to package
        files_to_create = {
            "imagecombiner.py": PYTHON_CODE_TEMPLATE,
            "requirements.txt": REQUIREMENTS_TXT,
            "README.md": README_MD
        }
        
        for filename, content in files_to_create.items():
            file_path = os.path.join(package_dir, filename)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"Created file: {file_path}")
        
        # Create zip file
        zip_name = f"ImageCombiner_{download_id}"
        zip_path = os.path.join(temp_dir, f"{zip_name}.zip")
        
        logger.info(f"Creating zip file: {zip_path}")
        shutil.make_archive(zip_path.replace('.zip', ''), 'zip', package_dir)
        
        # Verify zip file was created
        if not os.path.exists(zip_path):
            raise Exception(f"Zip file was not created: {zip_path}")
        
        file_size = os.path.getsize(zip_path)
        logger.info(f"Zip file created successfully, size: {file_size} bytes")
        
        # Store download info in memory
        downloads_db[download_id] = {
            "download_id": download_id,
            "name": request.name,
            "project_name": request.project_name,
            "zip_path": zip_path,
            "temp_dir": temp_dir,  # Store for cleanup
            "created_at": time.time(),
            "downloaded": False,
            "file_size": file_size
        }
        
        logger.info(f"Created download package: {download_id}")
        
        return DownloadResponse(
            download_id=download_id,
            message="Package created successfully! Ready for download."
        )
        
    except Exception as e:
        logger.error(f"Error creating download package: {e}")
        # Clean up temp directory if it was created
        if 'temp_dir' in locals():
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"Failed to create download package: {str(e)}")

@app.get("/api/download/{download_id}")
async def download_package(download_id: str):
    """Download the ImageCombiner package."""
    try:
        logger.info(f"Download requested for ID: {download_id}")
        
        # Find download info
        if download_id not in downloads_db:
            logger.error(f"Download ID not found: {download_id}")
            raise HTTPException(status_code=404, detail="Download not found")
        
        download_info = downloads_db[download_id]
        zip_path = download_info["zip_path"]
        
        logger.info(f"Zip path: {zip_path}")
        
        if not os.path.exists(zip_path):
            logger.error(f"Zip file not found: {zip_path}")
            raise HTTPException(status_code=404, detail="Download file not found")
        
        # Check file size
        file_size = os.path.getsize(zip_path)
        if file_size == 0:
            logger.error(f"Zip file is empty: {zip_path}")
            raise HTTPException(status_code=500, detail="Download file is corrupted")
        
        # Mark as downloaded
        downloads_db[download_id]["downloaded"] = True
        downloads_db[download_id]["downloaded_at"] = time.time()
        
        logger.info(f"Download started: {download_id}, file size: {file_size} bytes")
        
        # Return file with proper headers
        headers = {
            "Content-Disposition": f"attachment; filename=ImageCombiner_{download_id}.zip",
            "Content-Type": "application/zip",
            "Content-Length": str(file_size)
        }
        
        return FileResponse(
            zip_path,
            media_type="application/zip",
            filename=f"ImageCombiner_{download_id}.zip",
            headers=headers
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error downloading package: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to download package: {str(e)}")

@app.get("/api/download/{download_id}/status")
async def get_download_status(download_id: str):
    """Get download status and info."""
    try:
        if download_id not in downloads_db:
            raise HTTPException(status_code=404, detail="Download not found")
        
        download_info = downloads_db[download_id]
        zip_path = download_info["zip_path"]
        
        status = {
            "download_id": download_id,
            "created_at": download_info["created_at"],
            "downloaded": download_info.get("downloaded", False),
            "file_exists": os.path.exists(zip_path),
            "file_size": download_info.get("file_size", 0)
        }
        
        if download_info.get("downloaded"):
            status["downloaded_at"] = download_info.get("downloaded_at")
        
        return status
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting download status: {e}")
        raise HTTPException(status_code=500, detail="Failed to get download status")

@app.get("/api/stats")
async def get_stats():
    """Get download statistics."""
    try:
        total_downloads = len(downloads_db)
        completed_downloads = sum(1 for d in downloads_db.values() if d.get("downloaded", False))
        
        return {
            "total_packages_created": total_downloads,
            "completed_downloads": completed_downloads,
            "success_rate": (completed_downloads / total_downloads * 100) if total_downloads > 0 else 0,
            "active_downloads": list(downloads_db.keys())[-5:]  # Last 5 downloads
        }
        
    except Exception as e:
        logger.error(f"Error getting stats: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve stats")

@app.delete("/api/cleanup")
async def cleanup_old_downloads():
    """Clean up old download files (older than 1 hour)."""
    try:
        current_time = time.time()
        cleanup_count = 0
        
        downloads_to_remove = []
        for download_id, info in downloads_db.items():
            # Clean up files older than 1 hour
            if current_time - info["created_at"] > 3600:
                temp_dir = info.get("temp_dir")
                if temp_dir and os.path.exists(temp_dir):
                    try:
                        shutil.rmtree(temp_dir)
                        cleanup_count += 1
                        logger.info(f"Cleaned up temp directory: {temp_dir}")
                    except Exception as e:
                        logger.error(f"Failed to cleanup {temp_dir}: {e}")
                
                downloads_to_remove.append(download_id)
        
        # Remove from memory
        for download_id in downloads_to_remove:
            del downloads_db[download_id]
        
        return {
            "cleaned_files": cleanup_count,
            "removed_records": len(downloads_to_remove)
        }
        
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")
        raise HTTPException(status_code=500, detail="Failed to cleanup old downloads")

# Health check endpoint
@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": time.time()}

if __name__ == "__main__":
    import uvicorn
    print("üöÄ Starting ImageCombiner API...")
    print("üì± API will be available at: http://localhost:8000")
    print("üìö API docs at: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)